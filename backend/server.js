require('dotenv').config();
const express = require('express');
const axios = require('axios');
const cors = require('cors');

const db = require("./firebaseConfig"); // ğŸ”¹ Import Firestore setup
const admin = require("firebase-admin"); // ğŸ”¹ Needed for timestamp

const app = express();
app.use(express.json());
app.use(cors());

// Replace this with your actual API key
const PERSPECTIVE_API_KEY = process.env.PERSPECTIVE_API_KEY;

app.post("/moderate-text", async (req, res) => {
    try {
      const { text } = req.body;
  
      // ğŸ”¹ Send text to Perspective API for toxicity analysis
      const response = await axios.post(
        `https://commentanalyzer.googleapis.com/v1alpha1/comments:analyze?key=${PERSPECTIVE_API_KEY}`,
        {
          comment: { text },
          languages: ["en"],
          requestedAttributes: { TOXICITY: {} }, // ğŸ”¹ Checking TOXICITY attribute
        }
      );
  
      // ğŸ”¹ Extract toxicity score
      const toxicityScore = response.data.attributeScores.TOXICITY.summaryScore.value;
  
      // âœ… If toxicity is greater than 0.7, store in Firestore
      if (toxicityScore > 0.7) {
        await db.collection("flagged_content").add({
          type: "text", // ğŸ”¹ Indicating that this is flagged text
          content: text, // ğŸ”¹ Store the actual text
          toxicity_score: toxicityScore, // ğŸ”¹ Store toxicity score
          timestamp: admin.firestore.FieldValue.serverTimestamp(), // ğŸ”¹ Store time when flagged
        });
        console.log("Flagged content stored in Firestore");
      }
  
      // ğŸ”¹ Send response with the score
      res.json({ score: toxicityScore, flagged: toxicityScore > 0.7 });
    } catch (error) {
      console.error("Error in text moderation:", error);
      res.status(500).json({ error: "Error analyzing text" });
    }
  });
  
// Start the server
const PORT = process.env.PORT || 5001; 
app.listen(PORT, () => console.log(`ğŸš€ Server running on port ${PORT}`));
